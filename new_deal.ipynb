{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "这是我用来处理数据的文件，其完成的任务就是生成一个最终版本的cb_table表格用于文件main进行读取和学习。整体的cb_table表格的形状为100000x74（100000是指ratings中的行数），有73列是属性，其中第0列是userid，第1列是movieid，第2列是我爬取的数据metascore，第3-22列是第一类标签，第23列到第72列是第二类标签tags，最后一列是一个0.5的倍数：评分。第二类标签只有50列的原因是：我利用glove预处理模型将每个电影的tags处理成一个长度为50的向量，然后将其作为电影的50个属性进行学习。\n",
    "首先导入一些必要的库，其中torchtext库是用于文本处理、词嵌入的工作："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import torchtext as tt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "这些是处理数据必须用到的结构：将movieid转化成一个0-10000的数字，然后还需要设置一个从0-10000数字得到movieid的映射。这里的idToNum和NumToid就是这两个映射。cb_table作为全局变量，使得多个函数都可以对其进行修改。kind的含义之后会提及。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#global变量\n",
    "idToNum = {}#从id变为一个有序的序列\n",
    "NumToid = []#从具体的编号得到其id\n",
    "cb_table = 0#cb_table为最终用于训练的属性、评分,最终写入cb_table.csv文件中\n",
    "kind = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "第一个函数就是connect_id_Num，就是建立movieid和修正id之间的映射。然后这里涉及到了全局变量kind。kind的作用是将电影中的genres列提取出来，以免之后再次读取movies.csv文件。读取出来之后，可以让fill_cb_table函数直接处理。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def connect_id_Num():#创建idToNum和NumToid\n",
    "    global kind\n",
    "    movies = pd.read_csv(\"movies.csv\")\n",
    "    temp = np.array(movies[\"movieId\"])\n",
    "    for i in range(len(temp)):\n",
    "        idToNum[temp[i]] = i\n",
    "        NumToid.append(temp[i])\n",
    "    kind = list(movies[\"genres\"])#把全局变量kind存储下来，用于之后填充cb_table[:,3-22]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "build_cb_table函数的作用是初步搭建cb_table的一个框架，并填入一些基本的信息。在该函数中，首先创建了cb_table，并且读取了ans_table文件（爬取内容）。后续的操作是将userid、修正movieid、metascore和ratings先填入cb_table，之后便只剩下3-72行没有填入信息。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_cb_table():#初步搭建cb_table\n",
    "    global cb_table\n",
    "    ratings = np.array(pd.read_csv(\"ratings.csv\"))\n",
    "    cb_table = np.zeros((len(ratings),74),dtype = float)#返回结果\n",
    "\n",
    "    ans_table = np.array(pd.read_csv(\"ans_table.csv\"))#ans_table为爬取内容\n",
    "    for i in range(len(cb_table)):\n",
    "        cb_table[i,1] = idToNum[ratings[i,1]]#cb_table[:,1]为修正后的id\n",
    "        cb_table[i,2] = ans_table[idToNum[ratings[i,1]],4]#cb_table[:,2]为爬取的metascore分数\n",
    "    cb_table[:,0] = ratings[:,0]#cb_table[:,0]为用户id\n",
    "    cb_table[:,73] = ratings[:,2]#cb_table[:,23]为评分\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "fill_cb_table函数的作用是利用先前创建的全局变量kind，将每一个标签都编上一个序号。标签种类一共有20种，每一个标签当作一个属性，其中有这个标签则属性为1，否则为0。下面的操作是获得每一部电影的标签种类，并且对cb_table文件进行修改。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fill_cb_table():#把cb_table中空余的部分补充完整\n",
    "    S = set()#用于存放电影类型（str）的集合\n",
    "    for i in kind:#把所有电影类型拿出来，一共20部电影\n",
    "        x = i.split(\"|\")\n",
    "        for j in x:\n",
    "            S.add(j)\n",
    "    NumOfTag = {}#每一个Tag对应cb_table中的第几列\n",
    "    accum = 2#从2开始计数，3-22列为需要填充的部位\n",
    "    for i in S:\n",
    "        accum += 1\n",
    "        NumOfTag[i] = accum\n",
    "    for i in range(len(kind)):\n",
    "        Id = NumToid[i]\n",
    "        temp_index = np.where(cb_table[:,1] == Id)[0]\n",
    "        x = kind[i].split(\"|\")\n",
    "        for j in x:\n",
    "            cb_table[temp_index, NumOfTag[j]] = 1\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "这个函数的作用是：对导入cb_table的metascore进行处理。因为有的电影并没有metascore，这种时候我采取的填补缺失值策略是：覆盖上所有电影的平均值。此外，metascore的大小与其他属性的差距太大，因此这里我也对这一属性进行了标准化处理，将所有metascore归到均值为0、方差为1的正态分布中。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def deal_metascore():#处理爬取的metascore，进行填补缺失以及标准化处理\n",
    "    def normalize(x,st,d):\n",
    "        return (x-d)/st\n",
    "    s1 = sum(cb_table[:,2] != 0)\n",
    "    av1 = sum(cb_table[:,2]) / s1\n",
    "    cb_table[cb_table[:,2] == 0, 2] = av1#处理缺失项\n",
    "    st = np.std(cb_table[:,2])\n",
    "    d = np.mean(cb_table[:,2])\n",
    "    for i in range(len(cb_table)):\n",
    "        cb_table[i,2] = normalize(cb_table[i,2], st, d)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "这个函数是用于处理标签tags文件的函数，也是用到词嵌入方法的地方。\n",
    "在这里，我首先定义了一个embedding类，类中只有一个embedding层，这个embedding层是直接从预训练的glove模型中创建的，因为该模型太大因此并没有传入作业文件中，而如果本目录下没有该模型，glove模型实例化的语句会自动下载该模型。用预处理glove词库，我们可以将一个单词转化为一个50维的词向量。\n",
    "接下来是一个对tags预处理的函数first_deal。这个函数的功能是将一个字符串中的特殊字符（非数字，非字母）处理成空格，方便之后使用tokenizer进行分词。\n",
    "接下来就是初始化分词器tokenizer，并创建我自己的词表vocab。将vocab词表传入embedding便可实例化一个Embedding网络。\n",
    "接下来的操作是：对于一个tags（里面可能会有多个单词），对其中每一个单词都进行向量计算，最终将这些向量相加得到该tags的向量（虽然这会破坏单词的语义，但由于算力不够，只能进行此处理）。如果一部电影有多个tags，那么就对多个tags得到的向量进行平均值的求取而作为该电影的向量。如果没有被贴标签的电影，则向量直接置为0。\n",
    "得到向量以后，便可以对cb_table中的23-72行进行填补。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def deal_tags():\n",
    "    class Embedding(torch.nn.Module):#创建一个我的Embedding网络\n",
    "        def __init__(self,vocab):\n",
    "            super(Embedding,self).__init__()\n",
    "            glove = tt.vocab.GloVe(name='6B',dim=50)#实例化glove模型\n",
    "            self.embedding = torch.nn.Embedding.from_pretrained(\n",
    "                glove.get_vecs_by_tokens(vocab.get_itos()),#将词表和glove模型整合\n",
    "                freeze = True\n",
    "            )\n",
    "        def forward(self, a):\n",
    "            return self.embedding(a)\n",
    "    def first_deal(x):#字符串预处理，将非字母非数字的内容全部转化为空格\n",
    "        x = x.lower()\n",
    "        for i in range(len(x)):\n",
    "            if not (('0'<=x[i] and '9'>=x[i]) or ('a'<=x[i] and 'z'>=x[i])):\n",
    "                x = x[:i] + ' ' + x[i + 1:]\n",
    "        return x\n",
    "    tags = np.array(pd.read_csv(\"tags.csv\"))\n",
    "    tokenizer = tt.data.utils.get_tokenizer(\"basic_english\")#初始化分词器\n",
    "    word_list = [tokenizer(tags[i, 2]) for i in range(len(tags))]#分词器构建单词库\n",
    "    vocab = tt.vocab.build_vocab_from_iterator(word_list)#创建词表\n",
    "    E = Embedding(vocab)#初始化embedding网络\n",
    "    for i in range(len(tags)):\n",
    "        tags[i, 1] = idToNum[tags[i, 1]]#对于读入的tags表，先将movieid修正\n",
    "        tags[i, 2] = first_deal(tags[i, 2])#处理字符串\n",
    "    S = set(list(tags[:,1]))#获得所有被贴上标签的id集合\n",
    "    for i in S:#i是一个电影id\n",
    "        idx = np.where(tags[:,1] == i)[0]#找到tags里面id为i的索引\n",
    "        length = len(idx)#length表示idx的长度，也即电影i被贴了几个标签\n",
    "        temp = 0\n",
    "        for j in idx:#j是索引中的一个，有tags[j,1]==i\n",
    "            lt = word_list[j]#lt获取这个标签的分词列表（以空格分开）\n",
    "            lt = [vocab[x] for x in lt]#将每个字转化为他的索引\n",
    "            input_tensor = torch.tensor(lt)\n",
    "            input_tensor = input_tensor.unsqueeze(0)\n",
    "            output_tensor = E(input_tensor)\n",
    "            output_tensor = output_tensor.squeeze(0)\n",
    "            ttemp = 0\n",
    "            for x in output_tensor:#一个标签lt可能由很多个单词组成，这里将他们\n",
    "                ttemp += x\n",
    "            temp += ttemp\n",
    "        temp /= length\n",
    "        idx = np.where(cb_table[:, 1] == i)[0]\n",
    "        for j in idx:\n",
    "            for k in range(50):\n",
    "                cb_table[j, 23 + k] = temp[k]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "查看在connect_id_Num函数执行后一些数据结构的情况："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'dict'>\n",
      "<class 'list'>\n"
     ]
    }
   ],
   "source": [
    "connect_id_Num()\n",
    "print(type(idToNum))\n",
    "print(type(NumToid))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(5, 74)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[ 1.,  0., 96.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
       "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
       "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
       "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
       "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
       "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  4.],\n",
       "       [ 1.,  2., 46.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
       "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
       "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
       "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
       "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
       "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  4.],\n",
       "       [ 1.,  5., 76.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
       "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
       "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
       "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
       "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
       "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  4.],\n",
       "       [ 1., 43., 65.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
       "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
       "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
       "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
       "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
       "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  5.],\n",
       "       [ 1., 46., 76.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
       "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
       "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
       "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
       "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
       "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  5.]])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "build_cb_table()#初步搭建\n",
    "print(cb_table[:5].shape)\n",
    "cb_table[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 1.,  0., 96.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
       "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
       "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
       "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
       "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
       "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  4.],\n",
       "       [ 1.,  2., 46.,  0.,  0.,  0.,  1.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
       "         0.,  0.,  1.,  0.,  0.,  0.,  1.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
       "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
       "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
       "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
       "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  4.],\n",
       "       [ 1.,  5., 76.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
       "         0.,  0.,  0.,  0.,  0.,  1.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
       "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
       "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
       "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
       "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  4.],\n",
       "       [ 1., 43., 65.,  0.,  0.,  0.,  0.,  1.,  0.,  0.,  0.,  0.,  0.,\n",
       "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
       "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
       "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
       "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
       "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  5.],\n",
       "       [ 1., 46., 76.,  0.,  0.,  0.,  0.,  1.,  0.,  0.,  0.,  0.,  0.,\n",
       "         0.,  1.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
       "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
       "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
       "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
       "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  5.]])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fill_cb_table()#填补3-22列\n",
    "cb_table[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 1.        ,  0.        ,  1.79998256,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  4.        ],\n",
       "       [ 1.        ,  2.        , -1.19374806,  0.        ,  0.        ,\n",
       "         0.        ,  1.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         1.        ,  0.        ,  0.        ,  0.        ,  1.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  4.        ],\n",
       "       [ 1.        ,  5.        ,  0.60249031,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  1.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  4.        ]])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "deal_metascore()#处理metascore\n",
    "cb_table[:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 1.00000000e+00,  0.00000000e+00,  1.79998256e+00,\n",
       "         0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
       "         0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
       "         0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
       "         0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
       "         0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
       "         0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
       "         0.00000000e+00,  0.00000000e+00,  8.71853352e-01,\n",
       "        -1.04166996e+00, -6.92000007e-03,  4.73069996e-01,\n",
       "         3.33009988e-01, -2.81479985e-01, -8.56400013e-01,\n",
       "        -1.26901662e+00, -1.37054667e-01,  1.02336669e+00,\n",
       "        -5.56016684e-01,  7.61323392e-01, -8.25756609e-01,\n",
       "         4.16508317e-01,  7.99336672e-01, -1.85498998e-01,\n",
       "         1.36203334e-01,  9.56720054e-01, -3.66590023e-01,\n",
       "        -6.35653317e-01,  4.15416986e-01,  7.03486681e-01,\n",
       "        -2.02686667e-01, -5.29666757e-03,  2.43153334e-01,\n",
       "         1.69566676e-01, -1.03816664e+00, -5.08000469e-03,\n",
       "        -8.65266696e-02, -6.31426632e-01,  1.21988666e+00,\n",
       "         4.41586703e-01,  2.89489985e-01,  2.40290001e-01,\n",
       "         1.28689989e-01,  2.39956662e-01, -6.09369993e-01,\n",
       "        -2.62156636e-01, -2.45876670e-01, -1.27709663e+00,\n",
       "        -3.06603342e-01,  2.99796671e-01, -8.26719999e-01,\n",
       "        -3.94333415e-02, -4.76483345e-01,  1.98019996e-01,\n",
       "         5.96543312e-01, -2.17842653e-01, -3.81530017e-01,\n",
       "         9.50193346e-01,  4.00000000e+00],\n",
       "       [ 1.00000000e+00,  2.00000000e+00, -1.19374806e+00,\n",
       "         0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
       "         1.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
       "         0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
       "         0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
       "         1.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
       "         0.00000000e+00,  1.00000000e+00,  0.00000000e+00,\n",
       "         0.00000000e+00,  0.00000000e+00, -5.08744955e-01,\n",
       "         1.68750018e-01, -3.49184990e-01, -7.04829991e-01,\n",
       "         4.37420011e-01,  3.94749999e-01, -9.10979986e-01,\n",
       "        -6.39002770e-04, -4.64264989e-01,  2.70280004e-01,\n",
       "        -1.59100503e-01, -2.43539989e-01,  2.12054968e-01,\n",
       "         2.96195000e-01,  6.38180017e-01,  3.25830996e-01,\n",
       "        -3.93025011e-01,  3.88110012e-01,  6.71700001e-01,\n",
       "         3.29678386e-01, -4.74415004e-01,  3.69529992e-01,\n",
       "        -2.89534986e-01,  3.14949989e-01, -1.75690010e-01,\n",
       "        -8.99435043e-01, -3.95224988e-01,  9.88104999e-01,\n",
       "         9.52069998e-01, -1.71224996e-01,  1.07732499e+00,\n",
       "        -5.34995019e-01,  2.60260522e-01,  4.07835007e-01,\n",
       "         4.61804986e-01,  9.02270019e-01, -1.98449999e-01,\n",
       "         2.42864996e-01,  9.14554954e-01, -3.80280018e-01,\n",
       "        -3.58390003e-01,  1.39417008e-01,  1.09850019e-02,\n",
       "         1.59734994e-01,  4.26744998e-01,  4.28389996e-01,\n",
       "        -3.13425027e-02, -4.02114987e-01,  1.69665497e-02,\n",
       "        -6.20204985e-01,  4.00000000e+00]])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "deal_tags()#添加向量\n",
    "cb_table[:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(cb_table).to_csv(\"cb_table2.csv\")#存储cb_table"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
